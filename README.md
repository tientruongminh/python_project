# Walmart Product Review Analysis Pipeline

A comprehensive data analysis pipeline for Walmart product reviews with OOP architecture, web scraping, Gemini AI clustering, and aspect-based sentiment analysis.

##  Features

- **Data Loading**: Automatic loading from Kaggle with column normalization
- **Preprocessing**:  duplicate URL merging, missing value handling
- **Web Scraping**: Selenium-based scraper for Walmart product pages
- **AI Clustering**: Product categorization using Google Gemini API
- **Sentiment Analysis**: Aspect-based sentiment analysis of reviews
- **Business Insights**: Actionable recommendations based on analysis

##  Project Structure

```
python_project/
â”œâ”€â”€ .streamlit/
â”‚   â””â”€â”€ config.toml                          # Streamlit configuration
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw_data.csv                         # Raw Walmart reviews dataset
â”‚   â”œâ”€â”€ processed_data.csv                   # Cleaned dataset (version 1)
â”‚   â””â”€â”€ processed_data_v2.csv                # Cleaned dataset (version 2)
â”‚
â”œâ”€â”€ outputs/
â”‚   â”œâ”€â”€ analysis_report.md                   # Generated business insights report
â”‚   â”œâ”€â”€ clustered_products.csv               # Products with assigned categories
â”‚   â”œâ”€â”€ sentiment_analysis.csv               # Aspect-based sentiment results
â”‚   â””â”€â”€ pipeline.log                         # Pipeline execution logs
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py                          # Package initializer
â”‚   â”‚
â”‚   â”œâ”€â”€ analysis/
â”‚   â”‚   â”œâ”€â”€ __init__.py                      # Analysis module init
â”‚   â”‚   â”œâ”€â”€ aspect_extractor.py              # Extract aspects from reviews
â”‚   â”‚   â”œâ”€â”€ aspect_summarizer.py             # Summarize aspects per category
â”‚   â”‚   â”œâ”€â”€ evaluator.py                     # Model evaluation metrics
â”‚   â”‚   â”œâ”€â”€ insight_generator.py             # Generate business insights
â”‚   â”‚   â”œâ”€â”€ rag_pipeline.py                  # RAG-based Q&A pipeline
â”‚   â”‚   â”œâ”€â”€ sentiment_analyzer.py            # Sentiment classification
â”‚   â”‚   â””â”€â”€ topic_modeler.py                 # Topic modeling with LDA/BERTopic
â”‚   â”‚
â”‚   â”œâ”€â”€ clustering/
â”‚   â”‚   â”œâ”€â”€ __init__.py                      # Clustering module init
â”‚   â”‚   â”œâ”€â”€ gemini_client.py                 # Google Gemini API client
â”‚   â”‚   â””â”€â”€ product_clusterer.py             # Product categorization logic
â”‚   â”‚
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â”œâ”€â”€ __init__.py                      # Config module init
â”‚   â”‚   â””â”€â”€ settings.py                      # Environment & pipeline settings
â”‚   â”‚
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ __init__.py                      # Data module init
â”‚   â”‚   â”œâ”€â”€ loader.py                        # Kaggle data loader
â”‚   â”‚   â”œâ”€â”€ preprocessor.py                  # Data cleaning & transformation
â”‚   â”‚   â””â”€â”€ imputer.py                       # Missing value imputation
â”‚   â”‚
â”‚   â”œâ”€â”€ scrapers/
â”‚   â”‚   â”œâ”€â”€ __init__.py                      # Scrapers module init
â”‚   â”‚   â”œâ”€â”€ base_scraper.py                  # Abstract base scraper class
â”‚   â”‚   â””â”€â”€ walmart_scraper.py               # Walmart product page scraper
â”‚   â”‚
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ __init__.py                      # Utils module init
â”‚       â””â”€â”€ helpers.py                       # Common helper functions
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ __init__.py                          # Tests module init
â”‚   â””â”€â”€ test_preprocessor.py                 # Preprocessor tests
â”‚
â”œâ”€â”€ .env                                     # Environment variables (API keys)
â”œâ”€â”€ .gitignore                               # Git ignore rules
â”œâ”€â”€ main.py                                  # Pipeline entry point (CLI)
â”œâ”€â”€ streamlit_app.py                         # Interactive dashboard
â””â”€â”€ requirements.txt                         # Python dependencies
```

##  Installation

```bash
pip install -r requirements.txt
```

## âš™ï¸ Configuration

Set your Gemini API key:
```bash
export GEMINI_API_KEY="your-api-key-here"
```

Or create a `.env` file:
```
GEMINI_API_KEY=your-api-key-here
```

## ğŸ”§ Usage

### Full Pipeline
```bash
python main.py
```

### Step-by-step
```bash
python main.py --step load          # Load data only
python main.py --step preprocess    # Preprocess data
python main.py --step scrape        # Fill missing via scraping
python main.py --step cluster       # Cluster products
python main.py --step analyze       # Analyze reviews
python main.py --step report        # Generate report
```

##  Output

- `outputs/processed_data.csv` - Cleaned dataset
- `outputs/clustered_products.csv` - Products with categories
- `outputs/sentiment_analysis.csv` - Aspect-based sentiments
- `outputs/analysis_report.md` - Business insights report

## Methodology

1. **Data Preprocessing**
   - Rename columns to snake_case
   - Shift dates back 10 years
   - Merge duplicate PageURLs

2. **Missing Data Imputation**
   - Scrape Walmart pages via Selenium
   - Fallback: LLM inference for missing fields

3. **Product Clustering**
   - Extract product info from PageURLs
   - Use Gemini API to categorize products
   - Assign meaningful category names

4. **Aspect-Based Sentiment Analysis**
   - Extract aspects: quality, price, shipping, etc.
   - Analyze sentiment per aspect per category
   - Track sentiment trends over time

5. **Insight Generation**
   - Identify customer pain points
   - Generate actionable recommendations
   - Create business strategy suggestions

## ğŸ“„ License

MIT License
