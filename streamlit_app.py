"""
Streamlit Dashboard for Walmart Product Review Analysis.

Features:
- Comprehensive Project Report
- Gemini-powered Chatbot for natural language Q&A
"""
import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from pathlib import Path
import sys
import os

# Add project root to path
sys.path.insert(0, str(Path(__file__).parent))

from src.config.settings import settings, OUTPUT_DIR
from src.analysis.aspect_summarizer import AspectSummarizer

# Page config
st.set_page_config(
    page_title="Walmart Review Analysis",
    page_icon=None,
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS
st.markdown("""
<style>
    .main-header {
        font-size: 2rem;
        font-weight: bold;
        margin-bottom: 1rem;
    }
    .section-header {
        font-size: 1.5rem;
        font-weight: 600;
        margin-top: 1.5rem;
        margin-bottom: 0.5rem;
        border-bottom: 2px solid #1E88E5;
        padding-bottom: 0.3rem;
    }
    .subsection-header {
        font-size: 1.2rem;
        font-weight: 600;
        margin-top: 1rem;
        color: #424242;
    }
    .chat-container {
        background-color: #f8f9fa;
        padding: 1.5rem;
        border-radius: 0.5rem;
        margin: 1rem 0;
    }
    .user-message {
        background-color: #e3f2fd;
        padding: 1rem;
        border-radius: 0.5rem;
        margin: 0.5rem 0;
        border-left: 4px solid #1E88E5;
    }
    .bot-message {
        background-color: #ffffff;
        padding: 1rem;
        border-radius: 0.5rem;
        margin: 0.5rem 0;
        border-left: 4px solid #4CAF50;
        box-shadow: 0 1px 3px rgba(0,0,0,0.1);
    }
    .methodology-box {
        background-color: #e8f5e9;
        padding: 1rem;
        border-radius: 0.5rem;
        margin: 0.5rem 0;
    }
</style>
""", unsafe_allow_html=True)


@st.cache_data
def load_data():
    """Load processed data."""
    data_path = OUTPUT_DIR / "sentiment_analysis.csv"
    if data_path.exists():
        return pd.read_csv(data_path)
    
    data_path = settings.processed_data_path
    if data_path.exists():
        return pd.read_csv(data_path)
        
    return None


@st.cache_resource
def get_summarizer(df):
    """Get or create AspectSummarizer."""
    return AspectSummarizer(df)


@st.cache_resource
def get_gemini_client():
    """Initialize Gemini client for chatbot."""
    try:
        from src.clustering.gemini_client import GeminiClient
        client = GeminiClient()
        if client.is_available:
            return client
    except Exception as e:
        st.warning(f"Gemini not available: {e}")
    return None


def render_sidebar():
    """Render sidebar with navigation."""
    st.sidebar.markdown("## Menu")
    
    page = st.sidebar.radio(
        "Select Page",
        ["Bao Cao Du An", "Phan Tich Khia Canh", "RAG Query", "Danh Gia Mo Hinh"],
        label_visibility="collapsed"
    )
    
    st.sidebar.markdown("---")
    st.sidebar.markdown("### Thong Tin")
    st.sidebar.markdown(
        "He thong phan tich danh gia san pham Walmart "
        "su dung Embeddings, Clustering va LLM (Gemini)."
    )
    
    return page


# ============================================================
# PROJECT REPORT PAGE
# ============================================================

def render_project_report(df):
    """Render comprehensive project report."""
    st.markdown('<div class="main-header">Bao Cao Chi Tiet Du An Phan Tich Danh Gia San Pham Walmart</div>', unsafe_allow_html=True)
    
    # Table of Contents
    st.markdown("""
    **Muc Luc:**
    1. [Tong Quan Du An](#1-tong-quan-du-an)
    2. [Du Lieu va Tien Xu Ly](#2-du-lieu-va-tien-xu-ly)
    3. [Phan Tich Kham Pha Du Lieu (EDA)](#3-phan-tich-kham-pha-du-lieu-eda)
    4. [Mo Hinh va Phuong Phap](#4-mo-hinh-va-phuong-phap)
    5. [Ket Qua Phan Tich](#5-ket-qua-phan-tich)
    6. [Ket Luan va Khuyen Nghi](#6-ket-luan-va-khuyen-nghi)
    """)
    
    st.markdown("---")
    
    # Section 1: Project Overview
    render_section1_overview(df)
    
    # Section 2: Data & Preprocessing
    render_section2_preprocessing(df)
    
    # Section 3: EDA
    render_section3_eda(df)
    
    # Section 4: Models & Methods
    render_section4_models()
    
    # Section 5: Results
    render_section5_results(df)
    
    # Section 6: Conclusions
    render_section6_conclusions(df)


def render_section1_overview(df):
    """Section 1: Project Overview."""
    st.markdown('<div class="section-header">1. Tong Quan Du An</div>', unsafe_allow_html=True)
    
    st.markdown("""
    **Muc tieu:** Xay dung pipeline phan tich danh gia san pham tu Walmart de:
    - Hieu nguoi dung noi gi ve san pham
    - Phat hien cac khia canh (aspects) duoc de cap nhieu nhat
    - Phan tich cam xuc (sentiment) theo tung khia canh
    - Tao chatbot tra loi cau hoi bang ngon ngu tu nhien
    
    **Cong nghe su dung:**
    - Python 3.12, Pandas, Plotly
    - BERTopic cho topic modeling
    - Google Gemini API cho LLM
    - Streamlit cho dashboard
    """)
    
    # Key metrics
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("Tong So Danh Gia", f"{len(df):,}")
    
    with col2:
        unique_products = df['pageurl'].nunique() if 'pageurl' in df.columns else 0
        st.metric("So San Pham", f"{unique_products:,}")
    
    with col3:
        categories = df['product_category'].nunique() if 'product_category' in df.columns else 0
        st.metric("So Danh Muc", f"{categories:,}")
    
    with col4:
        avg_rating = df['rating'].mean() if 'rating' in df.columns else 0
        st.metric("Diem Trung Binh", f"{avg_rating:.2f}/5")


def render_section2_preprocessing(df):
    """Section 2: Data & Preprocessing."""
    st.markdown('<div class="section-header">2. D·ªØ Li·ªáu v√† Quy Trinh X·ª≠ L√Ω Chi Ti·∫øt</div>', unsafe_allow_html=True)
    
    st.markdown("### 2.1 D·ªØ Li·ªáu Ban ƒê·∫ßu (Raw Data)")
    st.markdown("""
    B·ªô d·ªØ li·ªáu ban ƒë·∫ßu bao g·ªìm **29,997 d√≤ng** ƒë√°nh gi√° s·∫£n ph·∫©m Walmart.
    
    **C√°c v·∫•n ƒë·ªÅ ch√≠nh ƒë∆∞·ª£c ph√°t hi·ªán:**
    1. **Thi·∫øu d·ªØ li·ªáu nghi√™m tr·ªçng (Completeness):**
       - C·ªôt `title` thi·∫øu 90.9% (27,276 d√≤ng).
       - C·ªôt `reviewer_name` thi·∫øu 5.4% (1,620 d√≤ng).
       - C√°c c·ªôt ph√¢n ph·ªëi sao (`five_star`, `one_star`...) thi·∫øu 0.3%.
    2. **D·ªØ li·ªáu r√°c/l·ªói (Accuracy):**
       - M·ªôt s·ªë `rating` n·∫±m ngo√†i kho·∫£ng [1, 5].
       - S·ªë l∆∞·ª£ng vote ti√™u c·ª±c (`negative_votes`) c√≥ gi√° tr·ªã √¢m.
    3. **ƒê·ªãnh d·∫°ng kh√¥ng nh·∫•t qu√°n (Validity):**
       - C·ªôt `verified_purchaser` ch·ª©a nhi·ªÅu gi√° tr·ªã ("Yes", "yes", "true", "True").
       - ƒê·ªãnh d·∫°ng ng√†y th√°ng kh√¥ng ƒë·ªìng nh·∫•t.
    4. **D∆∞ th·ª´a (Uniqueness):**
       - 355 d√≤ng b·ªã tr√πng l·∫∑p ho√†n to√†n.
       - URL ch·ª©a tham s·ªë tracking th·ª´a.
    """)
    
    st.markdown("### 2.2 C√°c B∆∞·ªõc X·ª≠ L√Ω (Data Cleaning Pipeline)")
    st.success("""
    **B∆∞·ªõc 1: Imputation (ƒêi·ªÅn d·ªØ li·ªáu thi·∫øu)**
    - **Product Title:** S·ª≠ d·ª•ng chi·∫øn l∆∞·ª£c **Product ID Matching**. T√¨m c√°c d√≤ng c√≥ c√πng Product ID (t·ª´ URL) nh∆∞ng c√≥ Title, sau ƒë√≥ copy Title sang c√°c d√≤ng b·ªã thi·∫øu. Fill ƒë∆∞·ª£c **10,345 titles**. S·ªë c√≤n l·∫°i g√°n "Unknown Product".
    - **Reviewer Name:** ƒêi·ªÅn gi√° tr·ªã m·∫∑c ƒë·ªãnh "Anonymous".
    - **Star Distribution:** ƒêi·ªÅn gi√° tr·ªã 0 cho c√°c ph√¢n ph·ªëi sao b·ªã thi·∫øu.
    
    **B∆∞·ªõc 2: Cleaning & Validation**
    - **Rating:** Clip gi√° tr·ªã v·ªÅ kho·∫£ng [1, 5].
    - **Text Cleaning:** Lo·∫°i b·ªè HTML tags, kho·∫£ng tr·∫Øng th·ª´a trong `review` v√† `title`.
    - **Normalization:** Chu·∫©n h√≥a `verified_purchaser` v·ªÅ Yes/No/Unknown.
    - **Duplicate Removal:** X√≥a 355 d√≤ng tr√πng l·∫∑p.
    
    **B∆∞·ªõc 3: Feature Engineering (T·∫°o ƒë·∫∑c tr∆∞ng m·ªõi)**
    - `helpfulness_score`: T√≠nh theo c√¥ng th·ª©c Wilson Score (c√¢n b·∫±ng gi·ªØa upvotes v√† total votes).
    - `sentiment_category`: Ph√¢n lo·∫°i d·ª±a tr√™n rating (4-5: Positive, 3: Neutral, 1-2: Negative).
    - `review_length`: ƒê·ªô d√†i ƒë√°nh gi√° (s·ªë t·ª´).
    """)
    
    st.info(f"**K·∫øt qu·∫£ sau x·ª≠ l√Ω:** B·ªô d·ªØ li·ªáu s·∫°ch g·ªìm **{len(df):,} d√≤ng**, s·∫µn s√†ng cho ph√¢n t√≠ch m√¥ h√¨nh.")


def render_section3_eda(df):
    """Section 3: EDA."""
    st.markdown('<div class="section-header">3. Ph√¢n T√≠ch Kh√°m Ph√° D·ªØ Li·ªáu (EDA)</div>', unsafe_allow_html=True)
    
    st.markdown("Trong ph·∫ßn n√†y, ch√∫ng ta s·∫Ω ƒëi s√¢u v√†o c√°c ƒë·∫∑c ƒëi·ªÉm th·ªëng k√™ c·ªßa d·ªØ li·ªáu ƒë·ªÉ hi·ªÉu r√µ h√†nh vi ng∆∞·ªùi d√πng.")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown('<div class="subsection-header">3.1 Ph√¢n B·ªë Rating (ƒêi·ªÉm ƒê√°nh Gi√°)</div>', unsafe_allow_html=True)
        if 'rating' in df.columns:
            rating_counts = df['rating'].value_counts().sort_index()
            fig = px.bar(
                x=rating_counts.index,
                y=rating_counts.values,
                labels={'x': 'Rating', 'y': 'S·ªë L∆∞·ª£ng Reviews'},
                color=rating_counts.values,
                color_continuous_scale='Blues'
            )
            fig.update_layout(showlegend=False, coloraxis_showscale=False, height=300)
            st.plotly_chart(fig, use_container_width=True)
            
            # Insights
            mode_rating = rating_counts.idxmax()
            st.info(f"""
            **Insight:**
            - Ph·∫ßn l·ªõn ƒë√°nh gi√° l√† **5 sao** ({rating_counts.max():,} reviews), chi·∫øm **{rating_counts.get(5, 0)/len(df)*100:.1f}%**.
            - ƒêi·ªÅu n√†y cho th·∫•y d·ªØ li·ªáu b·ªã l·ªách v·ªÅ ph√≠a t√≠ch c·ª±c (Positively Skewed), m·ªôt ƒë·∫∑c ƒëi·ªÉm chung c·ªßa E-commerce reviews.
            - Tuy nhi√™n, v·∫´n c√≥ **{rating_counts.get(1, 0):,}** ƒë√°nh gi√° 1 sao c·∫ßn l∆∞u √Ω.
            """)
    
    with col2:
        st.markdown('<div class="subsection-header">3.2 Ph√¢n B·ªë C·∫£m X√∫c (Sentiment)</div>', unsafe_allow_html=True)
        if 'rating_sentiment' in df.columns:
            sentiment_counts = df['rating_sentiment'].value_counts()
            fig = px.pie(
                values=sentiment_counts.values,
                names=sentiment_counts.index,
                color=sentiment_counts.index,
                color_discrete_map={
                    'Positive': '#4CAF50',
                    'Neutral': '#FFC107',
                    'Negative': '#F44336'
                }
            )
            fig.update_layout(height=300)
            st.plotly_chart(fig, use_container_width=True)
            
            pos_pct = sentiment_counts.get('Positive', 0) / len(df) * 100
            st.info(f"""
            **Insight:**
            - **{pos_pct:.1f}%** kh√°ch h√†ng h√†i l√≤ng v·ªõi s·∫£n ph·∫©m.
            - Ch·ªâ c√≥ m·ªôt t·ª∑ l·ªá nh·ªè ({sentiment_counts.get('Negative', 0)/len(df)*100:.1f}%) l√† ti√™u c·ª±c.
            - T·ª∑ l·ªá n√†y kh·∫≥ng ƒë·ªãnh l·∫°i xu h∆∞·ªõng t√≠ch c·ª±c c·ªßa t·∫≠p d·ªØ li·ªáu n√†y.
            """)
    
    # Category Analysis
    st.markdown('<div class="subsection-header">3.3 Hi·ªáu Su·∫•t Theo Danh M·ª•c</div>', unsafe_allow_html=True)
    if 'product_category' in df.columns:
        cat_agg = df.groupby('product_category').agg({
            'rating': ['count', 'mean'],
            'helpfulness_score': 'mean'
        }).reset_index()
        cat_agg.columns = ['Category', 'Reviews', 'Avg Rating', 'Avg Helpfulness']
        best_cats = cat_agg[cat_agg['Reviews'] > 50].sort_values('Avg Rating', ascending=False).head(5)
        worst_cats = cat_agg[cat_agg['Reviews'] > 50].sort_values('Avg Rating', ascending=True).head(5)
        
        c1, c2 = st.columns(2)
        with c1:
            st.markdown("**Top 5 Danh M·ª•c T·ªët Nh·∫•t (Rating cao nh·∫•t)**")
            st.table(best_cats[['Category', 'Avg Rating']].set_index('Category'))
            
        with c2:
            st.markdown("**Top 5 Danh M·ª•c C·∫ßn C·∫£i Thi·ªán (Rating th·∫•p nh·∫•t)**")
            st.table(worst_cats[['Category', 'Avg Rating']].set_index('Category'))
    
    # Time Trend
    st.markdown('<div class="subsection-header">3.4 Xu H∆∞·ªõng Theo Th·ªùi Gian</div>', unsafe_allow_html=True)
    if 'review_year_month' in df.columns:
        trend = df.groupby('review_year_month').agg({
            'rating': ['count', 'mean']
        }).reset_index()
        trend.columns = ['Th√°ng', 'Review Count', 'Avg Rating']
        trend = trend.sort_values('Th√°ng')
        
        fig = go.Figure()
        fig.add_trace(go.Bar(x=trend['Th√°ng'], y=trend['Review Count'], name='S·ªë L∆∞·ª£ng Reviews', marker_color='#90CAF9'))
        fig.add_trace(go.Scatter(x=trend['Th√°ng'], y=trend['Avg Rating'], name='Rating Trung B√¨nh', yaxis='y2', line=dict(color='#F44336', width=3)))
        
        fig.update_layout(
            title='Di·ªÖn Bi·∫øn Theo Th·ªùi Gian',
            yaxis=dict(title='Review Count'),
            yaxis2=dict(title='Avg Rating', overlaying='y', side='right', range=[3.5, 5]),
            height=400,
            legend=dict(orientation='h', y=1.1)
        )
        st.plotly_chart(fig, use_container_width=True)
        
        st.info("**Insight:** S·ªë l∆∞·ª£ng ƒë√°nh gi√° c√≥ xu h∆∞·ªõng tƒÉng v√†o c√°c th√°ng cu·ªëi nƒÉm (m√πa mua s·∫Øm), trong khi Rating trung b√¨nh gi·ªØ ·ªü m·ª©c ·ªïn ƒë·ªãnh.")


def render_section4_models():
    """Section 4: Models & Methods."""
    st.markdown('<div class="section-header">4. M√¥ H√¨nh v√† Ph∆∞∆°ng Ph√°p Ph√¢n T√≠ch</div>', unsafe_allow_html=True)
    
    st.markdown("""
    H·ªá th·ªëng s·ª≠ d·ª•ng k·∫øt h·ª£p gi·ªØa **Unsupervised Learning (Clustering)** v√† **Large Language Models (LLM)** ƒë·ªÉ hi·ªÉu s√¢u s·∫Øc n·ªôi dung ƒë√°nh gi√°.
    """)
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("### 4.1 Product Clustering (Ph√¢n Nh√≥m S·∫£n Ph·∫©m)")
        st.markdown("""
        **V·∫•n ƒë·ªÅ:** D·ªØ li·ªáu th√¥ c√≥ h√†ng ng√†n s·∫£n ph·∫©m nh∆∞ng danh m·ª•c kh√¥ng r√µ r√†ng.
        
        **Gi·∫£i ph√°p - Gemini Zero-shot Classification:**
        1. **Input:** Danh s√°ch t√™n s·∫£n ph·∫©m (Product Titles).
        2. **Process:** S·ª≠ d·ª•ng LLM (Gemini) ƒë·ªÉ t·ª± ƒë·ªông g√°n nh√£n danh m·ª•c d·ª±a tr√™n ng·ªØ nghƒ©a c·ªßa t√™n s·∫£n ph·∫©m. Kh√¥ng c·∫ßn training data.
        3. **Post-process:** Gom c√°c nh√≥m nh·ªè l·∫ª (<10 s·∫£n ph·∫©m) v√†o nh√≥m "Other".
        
        **K·∫øt qu·∫£:** T·∫°o ra c·∫•u tr√∫c danh m·ª•c r√µ r√†ng (Electronics, Health, Home...) gi√∫p ph√¢n t√≠ch drill-down hi·ªáu qu·∫£.
        """)
        
    with col2:
        st.markdown("### 4.2 Aspect Analysis (Ph√¢n T√≠ch Kh√≠a C·∫°nh)")
        st.markdown("""
        **V·∫•n ƒë·ªÅ:** Mu·ªën bi·∫øt kh√°ch h√†ng n√≥i g√¨ v·ªÅ c·ª• th·ªÉ t·ª´ng kh√≠a c·∫°nh (gi√°, ch·∫•t l∆∞·ª£ng, giao h√†ng).
        
        **Gi·∫£i ph√°p - Embedding + Clustering:**
        1. **Embedding:** S·ª≠ d·ª•ng `sentence-transformers` ƒë·ªÉ chuy·ªÉn ƒë·ªïi t·ª´ng review text th√†nh vector ng·ªØ nghƒ©a (384 chi·ªÅu).
        2. **Dimensionality Reduction:** D√πng **UMAP** ƒë·ªÉ gi·∫£m chi·ªÅu d·ªØ li·ªáu, gi·ªØ l·∫°i c·∫•u tr√∫c c·ª•c b·ªô.
        3. **Clustering:** D√πng **KMeans** ƒë·ªÉ gom nh√≥m c√°c reviews c√≥ n·ªôi dung t∆∞∆°ng t·ª± nhau -> M·ªói c·ª•m ƒë·∫°i di·ªán cho m·ªôt Kh√≠a C·∫°nh (Aspect).
        4. **Summarization:** D√πng **Gemini** ƒë·ªÉ ƒë·ªçc c√°c reviews trong c·ª•m v√† t√≥m t·∫Øt n·ªôi dung ch√≠nh + ph√¢n t√≠ch c·∫£m x√∫c.
        """)
    
    st.markdown("### 4.3 T·∫°i sao ph∆∞∆°ng ph√°p n√†y hi·ªáu qu·∫£?")
    st.info("""
    - **Kh√¥ng c·∫ßn d√°n nh√£n th·ªß c√¥ng:** Ti·∫øt ki·ªám th·ªùi gian v√† c√¥ng s·ª©c.
    - **Hi·ªÉu ng·ªØ nghƒ©a s√¢u:** Sentence Embeddings n·∫Øm b·∫Øt ƒë∆∞·ª£c √Ω nghƒ©a c√¢u vƒÉn t·ªët h∆°n t·ª´ kh√≥a ƒë∆°n l·∫ª (Keyword-based). V√≠ d·ª•: "sound is tinny" s·∫Ω ƒë∆∞·ª£c gom nh√≥m v·ªõi "bad audio quality" d√π kh√¥ng chung t·ª´ kh√≥a.
    - **T√≥m t·∫Øt t·ª± nhi√™n:** LLM sinh ra vƒÉn b·∫£n t√≥m t·∫Øt d·ªÖ ƒë·ªçc, thay v√¨ ch·ªâ ƒë∆∞a ra ƒë√°m m√¢y t·ª´ kh√≥a (Word Cloud).
    """)
def render_section5_results(df):
    """Section 5: Results."""
    st.markdown('<div class="section-header">5. K·∫øt Qu·∫£ Ph√¢n T√≠ch Chi Ti·∫øt</div>', unsafe_allow_html=True)
    
    st.markdown('<div class="subsection-header">5.1 C√°c Kh√≠a C·∫°nh ƒê∆∞·ª£c Quan T√¢m Nh·∫•t (Analyzed Aspects)</div>', unsafe_allow_html=True)
    st.write("D∆∞·ªõi ƒë√¢y l√† b·∫£ng t·ªïng h·ª£p c√°c kh√≠a c·∫°nh (aspects) ƒë∆∞·ª£c tr√≠ch xu·∫•t t·ª´ n·ªôi dung reviews v√† ch·ªâ s·ªë c·∫£m x√∫c t∆∞∆°ng ·ª©ng.")
    
    aspect_cols = [col for col in df.columns if col.startswith('has_')]
    if aspect_cols:
        aspect_data = []
        for col in aspect_cols:
            aspect = col.replace('has_', '')
            mentions = df[col].sum() if df[col].dtype == bool else (df[col] == True).sum()
            sent_col = f'{aspect}_sentiment'
            if sent_col in df.columns:
                pos_rate = (df.loc[df[col] == True, sent_col] == 'positive').mean() * 100 if mentions > 0 else 0
                neg_rate = (df.loc[df[col] == True, sent_col] == 'negative').mean() * 100 if mentions > 0 else 0
            else:
                pos_rate = neg_rate = 0
            
            aspect_data.append({
                'Kh√≠a C·∫°nh': aspect.capitalize(),
                'S·ªë L∆∞·ª£t ƒê·ªÅ C·∫≠p': int(mentions),
                'T·ª∑ L·ªá (%)': mentions/len(df)*100,
                '% T√≠ch C·ª±c': pos_rate,
                '% Ti√™u C·ª±c': neg_rate
            })
        
        aspect_df = pd.DataFrame(aspect_data).sort_values('S·ªë L∆∞·ª£t ƒê·ªÅ C·∫≠p', ascending=False)
        
        # Display as robust dataframe with progress bars
        st.dataframe(
            aspect_df.style.format({
                'T·ª∑ L·ªá (%)': '{:.1f}%',
                '% T√≠ch C·ª±c': '{:.1f}%',
                '% Ti√™u C·ª±c': '{:.1f}%',
                'S·ªë L∆∞·ª£t ƒê·ªÅ C·∫≠p': "{:,}"
            }).bar(subset=['% T√≠ch C·ª±c'], color='#4CAF50')
              .bar(subset=['% Ti√™u C·ª±c'], color='#F44336'),
            use_container_width=True
        )
        
        # Chart
        st.markdown("**Bi·ªÉu ƒë·ªì T∆∞∆°ng Quan: T·∫ßn su·∫•t vs C·∫£m X√∫c**")
        fig = px.scatter(
            aspect_df,
            x='S·ªë L∆∞·ª£t ƒê·ªÅ C·∫≠p',
            y='% Ti√™u C·ª±c',
            size='S·ªë L∆∞·ª£t ƒê·ªÅ C·∫≠p',
            color='Kh√≠a C·∫°nh',
            text='Kh√≠a C·∫°nh',
            title='Aspect Map: T·∫ßn su·∫•t c√†ng l·ªõn & C√†ng ti√™u c·ª±c l√† V·∫•n ƒê·ªÅ (G√≥c ph·∫£i tr√™n)'
        )
        fig.update_traces(textposition='top center')
        fig.update_layout(height=400, showlegend=False)
        st.plotly_chart(fig, use_container_width=True)
        
        st.info("""
        **Ph√¢n t√≠ch Bi·ªÉu ƒë·ªì:**
        - C√°c kh√≠a c·∫°nh n·∫±m ·ªü g√≥c **ph·∫£i tr√™n** (nhi·ªÅu ng∆∞·ªùi n√≥i ƒë√™n & t·ª∑ l·ªá ti√™u c·ª±c cao) l√† nh·ªØng "pain points" c·∫ßn ∆∞u ti√™n gi·∫£i quy·∫øt.
        - C√°c kh√≠a c·∫°nh ·ªü g√≥c **ph·∫£i d∆∞·ªõi** (nhi·ªÅu ng∆∞·ªùi khen & √≠t ti√™u c·ª±c) l√† th·∫ø m·∫°nh c·∫ßn ph√°t huy.
        """)


def render_section6_conclusions(df):
    """Section 6: Conclusions."""
    st.markdown('<div class="section-header">6. K·∫øt Lu·∫≠n & Khuy·∫øn Ngh·ªã Chi·∫øn L∆∞·ª£c</div>', unsafe_allow_html=True)
    
    st.markdown("D·ª±a tr√™n k·∫øt qu·∫£ ph√¢n t√≠ch d·ªØ li·ªáu v√† m√¥ h√¨nh h·ªçc m√°y, ch√∫ng t√¥i ƒë·ªÅ xu·∫•t c√°c khuy·∫øn ngh·ªã sau:")
    
    c1, c2, c3 = st.columns(3)
    
    with c1:
        st.error("**üõë ∆Øu Ti√™n Cao (Immediate Action)**")
        st.markdown("""
        1. **C·∫£i thi·ªán Quy Tr√¨nh Ki·ªÉm Tra Ch·∫•t L∆∞·ª£ng (Quality):**
           - `Quality` l√† v·∫•n ƒë·ªÅ b·ªã ph√†n n√†n nhi·ªÅu nh·∫•t.
           - C·∫ßn ki·ªÉm tra k·ªπ c√°c l√¥ h√†ng c√≥ t·ª∑ l·ªá ƒë·ªïi tr·∫£ cao.
        2. **ƒê√†o T·∫°o D·ªãch V·ª• Kh√°ch H√†ng:**
           - T·ª∑ l·ªá ti√™u c·ª±c ·ªü `Customer Service` ƒëang ·ªü m·ª©c b√°o ƒë·ªông.
           - C·∫ßn c·∫£i thi·ªán th·ªùi gian ph·∫£n h·ªìi v√† th√°i ƒë·ªô nh√¢n vi√™n.
        """)
        
    with c2:
        st.warning("**‚ö†Ô∏è ∆Øu Ti√™n Trung B√¨nh (Monitor)**")
        st.markdown("""
        1. **T·ªëi ∆Øu H√≥a V·∫≠n Chuy·ªÉn (Shipping):**
           - Kh√°ch h√†ng quan t√¢m nhi·ªÅu ƒë·∫øn t·ªëc ƒë·ªô giao h√†ng.
           - C·∫ßn l√†m vi·ªác v·ªõi ƒë·ªëi t√°c v·∫≠n chuy·ªÉn ƒë·ªÉ gi·∫£m th·ªùi gian ship.
        2. **C·∫≠p Nh·∫≠t M√¥ T·∫£ S·∫£n Ph·∫©m:**
           - M·ªôt s·ªë ph√†n n√†n v·ªÅ vi·ªác "Not as described". C·∫ßn r√† so√°t l·∫°i Content.
        """)
        
    with c3:
        st.success("**‚úÖ Duy Tr√¨ & Ph√°t Huy (Strengths)**")
        st.markdown("""
        1. **Chi·∫øn L∆∞·ª£c Gi√° (Price):**
           - Kh√°ch h√†ng r·∫•t h√†i l√≤ng v·ªÅ gi√° c·∫£ (`Price` c√≥ positive sentiment cao).
           - C√≥ th·ªÉ c√¢n nh·∫Øc tƒÉng nh·∫π gi√° ·ªü c√°c s·∫£n ph·∫©m premium.
        2. **ƒêa D·∫°ng H√≥a Danh M·ª•c:**
           - C√°c ng√†nh h√†ng `Electronics` v√† `Home` ƒëang tƒÉng tr∆∞·ªüng t·ªët.
        """)
    
    st.markdown("---")
    st.markdown('<div class="subsection-header">6.5 L·ªô Tr√¨nh Ph√°t Tri·ªÉn H·ªá Th·ªëng (Next Steps)</div>', unsafe_allow_html=True)
    st.markdown("""
    ƒê·ªÉ n√¢ng cao hi·ªáu qu·∫£ c·ªßa h·ªá th·ªëng ph√¢n t√≠ch, c√°c b∆∞·ªõc ti·∫øp theo bao g·ªìm:
    1. **Real-time Monitoring Dashboard:** X√¢y d·ª±ng dashboard theo d√µi tr·ª±c gian th·ª±c ƒë·ªÉ ph√°t hi·ªán kh·ªßng ho·∫£ng truy·ªÅn th√¥ng s·ªõm.
    2. **T√≠ch h·ª£p th√™m ngu·ªìn d·ªØ li·ªáu:** K·∫øt h·ª£p d·ªØ li·ªáu t·ª´ Facebook, Twitter ƒë·ªÉ c√≥ c√°i nh√¨n ƒëa chi·ªÅu (Social Listening).
    3. **Fine-tune LLM:** Hu·∫•n luy·ªán l·∫°i model Gemini tr√™n t·∫≠p d·ªØ li·ªáu domain-specific c·ªßa Walmart ƒë·ªÉ hi·ªÉu c√°c thu·∫≠t ng·ªØ chuy√™n ng√†nh t·ªët h∆°n.
    """)


# ============================================================
# CHATBOT PAGE - EMBEDDING-BASED ASPECT ANALYSIS
# ============================================================

def render_chatbot(df, summarizer):
    """Render giao di·ªán ph√¢n t√≠ch kh√≠a c·∫°nh v·ªõi Embeddings."""
    st.markdown('<div class="main-header">Ph√¢n T√≠ch Kh√≠a C·∫°nh ƒê√°nh Gi√° S·∫£n Ph·∫©m</div>', unsafe_allow_html=True)
    
    st.markdown("""
    H·ªá th·ªëng s·ª≠ d·ª•ng **Sentence Embeddings + Clustering** ƒë·ªÉ ph√¢n t√≠ch kh√≠a c·∫°nh t·ª´ c√°c ƒë√°nh gi√°.
    Kh√¥ng s·ª≠ d·ª•ng rule-based, ho√†n to√†n d·ª±a tr√™n ng·ªØ nghƒ©a (semantic).
    
    **Quy tr√¨nh:**
    1. L·ªçc reviews theo s·∫£n ph·∫©m/danh m·ª•c
    2. T·∫°o embeddings b·∫±ng Sentence Transformers
    3. Gi·∫£m chi·ªÅu b·∫±ng UMAP
    4. Gom c·ª•m b·∫±ng KMeans
    5. ƒê·∫∑t t√™n v√† t√≥m t·∫Øt b·∫±ng Gemini LLM
    """)
    
    st.markdown("---")
    
    # Ch·ªçn mode
    mode = st.radio(
        "Ch·ªçn ch·∫ø ƒë·ªô ph√¢n t√≠ch:",
        ["Case 1: Ph√°t hi·ªán N kh√≠a c·∫°nh ph·ªï bi·∫øn", "Case 2: Ph√¢n t√≠ch theo t√™n kh√≠a c·∫°nh"],
        horizontal=True
    )
    
    st.markdown("---")
    
    if mode == "Case 1: Ph√°t hi·ªán N kh√≠a c·∫°nh ph·ªï bi·∫øn":
        render_case1_n_aspects(df, summarizer)
    else:
        render_case2_aspect_name(df, summarizer)


def render_case1_n_aspects(df, summarizer):
    """Case 1: Nh·∫≠p s·∫£n ph·∫©m/danh m·ª•c + s·ªë kh√≠a c·∫°nh."""
    st.markdown("### Case 1: Ph√°t Hi·ªán N Kh√≠a C·∫°nh Ph·ªï Bi·∫øn")
    
    st.markdown("""
    **Workflow:**
    1. L·ªçc reviews theo s·∫£n ph·∫©m ho·∫∑c danh m·ª•c
    2. Embedding t·∫•t c·∫£ reviews
    3. Gi·∫£m chi·ªÅu (UMAP) ‚Üí Gom c·ª•m (KMeans v·ªõi k = N)
    4. ƒê·∫∑t t√™n cho t·ª´ng cluster b·∫±ng LLM
    5. T√≥m t·∫Øt t·ª´ng kh√≠a c·∫°nh
    """)
    
    # Input fields
    col1, col2 = st.columns(2)
    
    with col1:
        # Ch·ªçn danh m·ª•c
        categories = ['T·∫•t c·∫£']
        if 'product_category' in df.columns:
            cats = df['product_category'].dropna().unique().tolist()
            cats = [c for c in cats if c not in ['Unknown', 'Other']]
            categories += sorted(cats)
        
        selected_category = st.selectbox("Ch·ªçn danh m·ª•c s·∫£n ph·∫©m:", categories)
        
        # Ho·∫∑c nh·∫≠p t√™n s·∫£n ph·∫©m
        product_name = st.text_input(
            "Ho·∫∑c nh·∫≠p t√™n s·∫£n ph·∫©m:",
            placeholder="V√≠ d·ª•: headphones, TV, tablet..."
        )
    
    with col2:
        n_aspects = st.number_input(
            "S·ªë kh√≠a c·∫°nh mu·ªën ph√°t hi·ªán:",
            min_value=2,
            max_value=10,
            value=3,
            help="S·ªë c·ª•m (clusters) s·∫Ω ƒë∆∞·ª£c t·∫°o"
        )
        
        max_reviews = st.number_input(
            "S·ªë reviews t·ªëi ƒëa:",
            min_value=50,
            max_value=1000,
            value=300,
            help="Gi·ªõi h·∫°n s·ªë reviews ƒë·ªÉ x·ª≠ l√Ω nhanh h∆°n"
        )
    
    if st.button("Ph√¢n T√≠ch Kh√≠a C·∫°nh", type="primary", key="case1_btn"):
        category = None if selected_category == "T·∫•t c·∫£" else selected_category
        product = product_name if product_name.strip() else None
        
        if not category and not product:
            st.warning("Vui l√≤ng ch·ªçn danh m·ª•c ho·∫∑c nh·∫≠p t√™n s·∫£n ph·∫©m.")
            return
        
        with st.spinner("ƒêang ph√¢n t√≠ch... (c√≥ th·ªÉ m·∫•t 1-2 ph√∫t)"):
            try:
                # Import EmbeddingAspectSummarizer tr·ª±c ti·∫øp
                from src.analysis.aspect_summarizer import EmbeddingAspectSummarizer
                
                # Ki·ªÉm tra xem c√≥ th·ªÉ s·ª≠ d·ª•ng embedding kh√¥ng
                embedding_summarizer = EmbeddingAspectSummarizer(df)
                
                result = embedding_summarizer.analyze_by_num_aspects(
                    n_aspects=int(n_aspects),
                    product=product,
                    category=category,
                    max_reviews=int(max_reviews)
                )
                
                display_case1_result(result)
                
            except Exception as e:
                st.error(f"L·ªói ph√¢n t√≠ch: {str(e)}")
                st.info("ƒê·∫£m b·∫£o ƒë√£ c√†i ƒë·∫∑t: pip install sentence-transformers umap-learn scikit-learn")


def display_case1_result(result):
    """Hi·ªÉn th·ªã k·∫øt qu·∫£ Case 1."""
    if not result.get('success'):
        st.error(result.get('error', 'C√≥ l·ªói x·∫£y ra'))
        return
    
    st.markdown("---")
    st.markdown(f"### K·∫øt Qu·∫£ Ph√¢n T√≠ch: {result.get('n_aspects')} Kh√≠a C·∫°nh")
    
    # Th√¥ng tin t·ªïng quan
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("T·ªïng Reviews Ph√¢n T√≠ch", f"{result.get('total_reviews', 0):,}")
    with col2:
        st.metric("S·ªë Kh√≠a C·∫°nh", result.get('n_aspects', 0))
    with col3:
        if result.get('category'):
            st.metric("Danh M·ª•c", result.get('category', 'N/A'))
        elif result.get('product'):
            st.metric("S·∫£n Ph·∫©m", result.get('product', 'N/A'))
            
    # HI·ªÇN TH·ªä T√ìM T·∫ÆT CHUNG (M·ªöI)
    if result.get('overall_summary'):
        st.info(f"**T√≥m T·∫Øt T·ªïng Quan:**\n\n{result.get('overall_summary')}")
    
    st.markdown("---")
    st.markdown("### Chi Ti·∫øt T·ª´ng Kh√≠a C·∫°nh")
    
    # Hi·ªÉn th·ªã t·ª´ng kh√≠a c·∫°nh
    for aspect in result.get('aspects', []):
        with st.container():
            st.markdown(f"#### Kh√≠a c·∫°nh {aspect['aspect_id']}: {aspect['aspect_name']}")
            
            # Layout: Metrics b√™n tr√°i, Summary b√™n ph·∫£i
            col1, col2 = st.columns([1, 2])
            
            with col1:
                st.metric("S·ªë Reviews Discussed", aspect['review_count'])
                
                # Sentiment Chart nh·ªè
                sentiment = aspect.get('sentiment', {})
                sent_data = {
                    'Positive': sentiment.get('positive_pct', 0),
                    'Neutral': sentiment.get('neutral_pct', 0),
                    'Negative': sentiment.get('negative_pct', 0)
                }
                st.write("**C·∫£m x√∫c:**")
                st.progress(sent_data['Positive']/100, text=f"Positive: {sent_data['Positive']}%")
                st.progress(sent_data['Negative']/100, text=f"Negative: {sent_data['Negative']}%")
            
            with col2:
                st.markdown(f"**T√≥m t·∫Øt ƒë·∫°i di·ªán:**")
                st.success(aspect['summary'])
            
            # Sample reviews
            if aspect.get('sample_reviews'):
                with st.expander(f"Xem ƒë√°nh gi√° chi ti·∫øt v·ªÅ {aspect['aspect_name']}"):
                    for i, review in enumerate(aspect['sample_reviews'][:5], 1):
                        st.markdown(f"**{i}.** {review}")
            
            st.markdown("---")


def render_case2_aspect_name(df, summarizer):
    """Case 2: Nh·∫≠p s·∫£n ph·∫©m + t√™n kh√≠a c·∫°nh."""
    st.markdown("### Case 2: Ph√¢n T√≠ch Theo T√™n Kh√≠a C·∫°nh")
    
    st.markdown("""
    **Workflow:**
    1. L·ªçc reviews theo s·∫£n ph·∫©m ho·∫∑c danh m·ª•c
    2. Embedding reviews + embedding t√™n kh√≠a c·∫°nh
    3. T√≠nh cosine similarity gi·ªØa reviews v√† kh√≠a c·∫°nh
    4. L·ªçc reviews c√≥ similarity cao
    5. T√≥m t·∫Øt c√°c reviews ƒë√≥ b·∫±ng LLM
    """)
    
    # Input fields
    col1, col2 = st.columns(2)
    
    with col1:
        # Ch·ªçn danh m·ª•c
        categories = ['T·∫•t c·∫£']
        if 'product_category' in df.columns:
            cats = df['product_category'].dropna().unique().tolist()
            cats = [c for c in cats if c not in ['Unknown', 'Other']]
            categories += sorted(cats)
        
        selected_category = st.selectbox(
            "Ch·ªçn danh m·ª•c s·∫£n ph·∫©m:", 
            categories,
            key="case2_category"
        )
        
        # Ho·∫∑c nh·∫≠p t√™n s·∫£n ph·∫©m
        product_name = st.text_input(
            "Ho·∫∑c nh·∫≠p t√™n s·∫£n ph·∫©m:",
            placeholder="V√≠ d·ª•: headphones, TV, tablet...",
            key="case2_product"
        )
    
    with col2:
        aspect_name = st.text_input(
            "Nh·∫≠p t√™n kh√≠a c·∫°nh mu·ªën ph√¢n t√≠ch:",
            placeholder="V√≠ d·ª•: sound quality, battery life, shipping speed...",
            help="H·ªá th·ªëng s·∫Ω t√¨m c√°c reviews c√≥ ng·ªØ nghƒ©a t∆∞∆°ng t·ª±"
        )
        
        similarity_threshold = st.slider(
            "Ng∆∞·ª°ng similarity t·ªëi thi·ªÉu:",
            min_value=0.1,
            max_value=0.8,
            value=0.3,
            step=0.05,
            help="Ch·ªâ l·∫•y reviews c√≥ similarity >= ng∆∞·ª°ng n√†y"
        )
    
    if st.button("Ph√¢n T√≠ch Kh√≠a C·∫°nh", type="primary", key="case2_btn"):
        if not aspect_name.strip():
            st.warning("Vui l√≤ng nh·∫≠p t√™n kh√≠a c·∫°nh.")
            return
        
        category = None if selected_category == "T·∫•t c·∫£" else selected_category
        product = product_name if product_name.strip() else None
        
        with st.spinner("ƒêang ph√¢n t√≠ch... (c√≥ th·ªÉ m·∫•t 1-2 ph√∫t)"):
            try:
                from src.analysis.aspect_summarizer import EmbeddingAspectSummarizer
                
                embedding_summarizer = EmbeddingAspectSummarizer(df)
                
                result = embedding_summarizer.analyze_by_aspect_name(
                    aspect_name=aspect_name,
                    product=product,
                    category=category,
                    similarity_threshold=similarity_threshold
                )
                
                display_case2_result(result)
                
            except Exception as e:
                st.error(f"L·ªói ph√¢n t√≠ch: {str(e)}")
                st.info("ƒê·∫£m b·∫£o ƒë√£ c√†i ƒë·∫∑t: pip install sentence-transformers scikit-learn")


def display_case2_result(result):
    """Hi·ªÉn th·ªã k·∫øt qu·∫£ Case 2."""
    if not result.get('success'):
        st.error(result.get('error', 'C√≥ l·ªói x·∫£y ra'))
        return
    
    st.markdown("---")
    st.markdown(f"### K·∫øt Qu·∫£ Ph√¢n T√≠ch: Kh√≠a C·∫°nh \"{result.get('aspect_name')}\"")
    
    # Overview Summary (C√°i chung nh·∫•t)
    # st.markdown("#### 1. T·ªïng Quan")
    st.info(f"**T√≥m T·∫Øt Kh√≠a C·∫°nh:**\n\n{result.get('summary', 'Kh√¥ng c√≥ t√≥m t·∫Øt')}")
    
    # Sentiment Analysis Overview
    sentiment = result.get('sentiment', {})
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("Sentiment Positive", f"{sentiment.get('positive_pct', 0)}%")
    with col2:
        st.metric("Sentiment Neutral", f"{sentiment.get('neutral_pct', 0)}%")
    with col3:
        st.metric("Sentiment Negative", f"{sentiment.get('negative_pct', 0)}%")
        
    st.markdown("---")
    
    # Details (C√°i ri√™ng)
    st.markdown("#### 2. Chi Ti·∫øt Ph√¢n T√≠ch")
    
    col1, col2 = st.columns(2)
    with col1:
        st.metric("T·ªïng Reviews ƒê√£ Qu√©t", f"{result.get('total_reviews_analyzed', 0):,}")
    with col2:
        st.metric("Reviews Li√™n Quan Found", f"{result.get('relevant_reviews_count', 0):,}")
        
    st.markdown("#### C√°c ƒê√°nh Gi√° ƒêi·ªÉn H√¨nh Nh·∫•t")
    st.caption("C√°c ƒë√°nh gi√° ƒë∆∞·ª£c s·∫Øp x·∫øp theo ƒë·ªô t∆∞∆°ng ƒë·ªìng ng·ªØ nghƒ©a (Semantic Similarity)")
    
    sample_reviews = result.get('sample_reviews', [])
    if sample_reviews:
        for i, item in enumerate(sample_reviews, 1):
            review = item.get('review', '')
            similarity = item.get('similarity', 0)
            
            with st.container():
                st.markdown(f"**Review #{i}** (Similarity: {similarity:.3f})")
                st.markdown(f"> {review}")
                st.markdown("")
    else:
        st.info("Kh√¥ng t√¨m th·∫•y reviews li√™n quan.")


# ============================================================
# MODEL EVALUATION PAGE
# ============================================================

def render_evaluation(df, summarizer):
    """Render model evaluation page."""
    st.markdown('<div class="main-header">ƒê√°nh Gi√° Ch·∫•t L∆∞·ª£ng M√¥ H√¨nh</div>', unsafe_allow_html=True)
    
    st.markdown("""
    Trang n√†y cho ph√©p ƒë√°nh gi√° ch·∫•t l∆∞·ª£ng c·ªßa m√¥ h√¨nh Aspect-Based Summarization.
    
    **C√°c nh√≥m metrics:**
    1. **Clustering Quality**: Silhouette Score, Calinski-Harabasz, Davies-Bouldin
    2. **Topic Coherence**: Semantic similarity trong t·ª´ng cluster
    3. **Coverage**: T·ª∑ l·ªá reviews ƒë∆∞·ª£c g√°n aspect
    4. **Summary Quality**: Relevance c·ªßa summary v·ªõi source reviews
    """)
    
    st.markdown("---")
    
    # Input parameters
    col1, col2 = st.columns(2)
    
    with col1:
        categories = ['Electronics - Headphones', 'Electronics - TV']
        if 'product_category' in df.columns:
            cats = df['product_category'].dropna().unique().tolist()
            cats = [c for c in cats if c not in ['Unknown', 'Other'] and len(df[df['product_category'] == c]) >= 50]
            if cats:
                categories = sorted(cats)[:20]  # Top 20 categories
        
        selected_category = st.selectbox(
            "Ch·ªçn danh m·ª•c ƒë·ªÉ ƒë√°nh gi√°:", 
            categories,
            key="eval_category"
        )
    
    with col2:
        n_aspects = st.number_input(
            "S·ªë kh√≠a c·∫°nh (clusters):",
            min_value=2,
            max_value=10,
            value=3,
            key="eval_n_aspects"
        )
    
    if st.button("Ch·∫°y ƒê√°nh Gi√°", type="primary", key="run_eval"):
        with st.spinner("ƒêang ƒë√°nh gi√° m√¥ h√¨nh... (c√≥ th·ªÉ m·∫•t 1-2 ph√∫t)"):
            try:
                from src.analysis.evaluator import AspectModelEvaluator
                from src.analysis.aspect_summarizer import EmbeddingAspectSummarizer
                
                # Create summarizer
                emb_summarizer = EmbeddingAspectSummarizer(df, fast_mode=True, use_cache=True)
                
                # Run analysis
                result = emb_summarizer.analyze_by_num_aspects(
                    n_aspects=int(n_aspects),
                    category=selected_category,
                    max_reviews=200
                )
                
                if not result.get('success'):
                    st.error(result.get('error', 'Ph√¢n t√≠ch th·∫•t b·∫°i'))
                    return
                
                # Get data for evaluation
                filtered_df = emb_summarizer._get_reviews_for_product(category=selected_category, max_reviews=200)
                reviews = filtered_df['review'].tolist()
                
                if len(reviews) < n_aspects:
                    st.error(f"Kh√¥ng ƒë·ªß reviews ({len(reviews)})")
                    return
                
                embeddings = emb_summarizer._create_embeddings(reviews)
                reduced = emb_summarizer._reduce_dimensions(embeddings)
                labels = emb_summarizer._cluster_embeddings(reduced, int(n_aspects))
                
                # Build clusters and summaries dicts
                clusters = {}
                summaries = {}
                for i, label in enumerate(labels):
                    if label not in clusters:
                        clusters[label] = []
                    clusters[label].append(reviews[i])
                
                for aspect in result['aspects']:
                    cluster_id = aspect['aspect_id'] - 1
                    summaries[cluster_id] = aspect['summary']
                
                # Run evaluation
                evaluator = AspectModelEvaluator(emb_summarizer.embedding_model)
                evaluation = evaluator.run_full_evaluation(
                    embeddings=reduced,
                    labels=labels,
                    clusters=clusters,
                    summaries=summaries,
                    total_reviews=len(reviews)
                )
                
                # Display results
                display_evaluation_results(evaluation, result)
                
            except ImportError as e:
                st.error(f"Thi·∫øu dependencies: {e}")
                st.info("C√†i ƒë·∫∑t: pip install sentence-transformers scikit-learn")
            except Exception as e:
                st.error(f"L·ªói: {str(e)}")
                import traceback
                st.code(traceback.format_exc())


def display_evaluation_results(evaluation: dict, analysis_result: dict):
    """Display evaluation results."""
    st.markdown("---")
    st.markdown("## K·∫øt Qu·∫£ ƒê√°nh Gi√°")
    
    # Overall Score
    overall = evaluation.get('overall_assessment', {})
    
    col1, col2, col3 = st.columns(3)
    with col1:
        score = overall.get('overall_score', 0)
        st.metric("Overall Score", f"{score:.1%}")
    with col2:
        st.metric("Grade", overall.get('grade', 'N/A'))
    with col3:
        st.metric("T·ªïng Reviews", evaluation.get('coverage', {}).get('total_reviews', 0))
    
    st.markdown("---")
    
    # Component scores
    st.markdown("### ƒêi·ªÉm Theo Th√†nh Ph·∫ßn")
    
    components = overall.get('components', {})
    comp_df = pd.DataFrame([
        {'Th√†nh ph·∫ßn': k.capitalize(), 'ƒêi·ªÉm': f"{v:.1%}"}
        for k, v in components.items()
    ])
    st.dataframe(comp_df, use_container_width=True, hide_index=True)
    
    st.markdown("---")
    
    # Detailed metrics
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("### 1. Clustering Quality")
        cluster = evaluation.get('clustering_quality', {})
        st.markdown(f"""
        - **Silhouette Score**: {cluster.get('silhouette_score', 0):.4f}
          - {cluster.get('silhouette_interpretation', 'N/A')}
        - **Calinski-Harabasz**: {cluster.get('calinski_harabasz', 0):.2f}
        - **Davies-Bouldin**: {cluster.get('davies_bouldin', 0):.4f}
          - {cluster.get('davies_interpretation', 'N/A')}
        """)
        
        st.markdown("### 2. Topic Coherence")
        coherence = evaluation.get('topic_coherence', {})
        st.markdown(f"""
        - **Overall Coherence**: {coherence.get('overall_coherence', 0):.4f}
        - **Interpretation**: {coherence.get('interpretation', 'N/A')}
        """)
    
    with col2:
        st.markdown("### 3. Coverage")
        coverage = evaluation.get('coverage', {})
        st.markdown(f"""
        - **T·ªïng reviews**: {coverage.get('total_reviews', 0):,}
        - **Reviews c√≥ aspect**: {coverage.get('reviews_with_aspects', 0):,}
        - **Coverage Rate**: {coverage.get('coverage_rate', 0):.1f}%
        - **Interpretation**: {coverage.get('coverage_interpretation', 'N/A')}
        """)
        
        if coverage.get('cluster_balance'):
            balance = coverage['cluster_balance']
            st.markdown(f"""
            **Cluster Balance:**
            - Min: {balance.get('min_size', 0)}, Max: {balance.get('max_size', 0)}
            - CV: {balance.get('coefficient_of_variation', 0):.3f}
            - {balance.get('balance_interpretation', 'N/A')}
            """)
        
        st.markdown("### 4. Summary Quality")
        summary = evaluation.get('summary_quality', {})
        st.markdown(f"""
        - **Avg Relevance**: {summary.get('avg_relevance', 0):.4f}
        - **Summaries evaluated**: {summary.get('n_summaries_evaluated', 0)}
        """)
    
    st.markdown("---")
    
    # Interpretation
    st.markdown("### Gi·∫£i Th√≠ch Metrics")
    
    with st.expander("Xem chi ti·∫øt v·ªÅ c√°c metrics"):
        st.markdown("""
        #### Clustering Quality
        
        | Metric | √ù nghƒ©a | Gi√° tr·ªã t·ªët |
        |--------|---------|-------------|
        | Silhouette Score | ƒê·ªô t√°ch bi·ªát gi·ªØa clusters | ‚â• 0.5 |
        | Calinski-Harabasz | T·ª∑ l·ªá variance between/within | C√†ng cao c√†ng t·ªët |
        | Davies-Bouldin | ƒê·ªô overlap gi·ªØa clusters | ‚â§ 1.0 |
        
        #### Topic Coherence
        
        ƒêo l∆∞·ªùng ƒë·ªô m·∫°ch l·∫°c c·ªßa c√°c topics/aspects ƒë∆∞·ª£c ph√°t hi·ªán.
        - **> 0.6**: R·∫•t t·ªët - Topics r√µ r√†ng, m·∫°ch l·∫°c
        - **0.4-0.6**: T·ªët - Topics c√≥ √Ω nghƒ©a
        - **0.25-0.4**: Trung b√¨nh - C√≥ overlap
        - **< 0.25**: Y·∫øu - Topics kh√¥ng r√µ r√†ng
        
        #### Coverage
        
        T·ª∑ l·ªá reviews ƒë∆∞·ª£c g√°n v√†o √≠t nh·∫•t 1 aspect.
        - **> 70%**: T·ªët
        - **40-70%**: Trung b√¨nh
        - **< 40%**: Y·∫øu
        
        #### Summary Quality
        
        ƒê·ªô li√™n quan gi·ªØa summary v√† source reviews.
        - **> 0.7**: R·∫•t t·ªët
        - **0.5-0.7**: T·ªët
        - **0.3-0.5**: Trung b√¨nh
        - **< 0.3**: Y·∫øu
        """)
    
    # Aspects found
    st.markdown("---")
    st.markdown("### C√°c Kh√≠a C·∫°nh ƒê∆∞·ª£c Ph√°t Hi·ªán")
    
    for aspect in analysis_result.get('aspects', []):
        with st.expander(f"Kh√≠a c·∫°nh {aspect['aspect_id']}: {aspect['aspect_name']} ({aspect['review_count']} reviews)"):
            st.markdown(f"**T√≥m t·∫Øt:** {aspect['summary']}")
            st.markdown("**Sample reviews:**")
            for i, rev in enumerate(aspect.get('sample_reviews', [])[:3], 1):
                st.markdown(f"{i}. {rev[:200]}...")


# ============================================================
# RAG QUERY PAGE
# ============================================================

def render_rag_query(df):
    """Render RAG query page."""
    st.markdown('<div class="main-header">RAG Query - Truy V·∫•n Nhanh</div>', unsafe_allow_html=True)
    
    st.markdown("""
    S·ª≠ d·ª•ng RAG (Retrieval-Augmented Generation) ƒë·ªÉ truy v·∫•n nhanh c√°c kh√≠a c·∫°nh ƒë√£ ƒë∆∞·ª£c pre-compute.
    
    **Workflow:**
    1. Pre-compute: Ph√¢n t√≠ch t·∫•t c·∫£ categories v√† l∆∞u v√†o vector store
    2. Query: T√¨m ki·∫øm semantic v√† generate response v·ªõi LLM
    """)
    
    st.markdown("---")
    
    # Initialize components
    try:
        from src.analysis.rag_pipeline import create_rag_pipeline
        vector_store, query_engine, precompute_pipeline = create_rag_pipeline(df)
    except Exception as e:
        st.error(f"Kh√¥ng th·ªÉ kh·ªüi t·∫°o RAG pipeline: {e}")
        return
    
    # Stats
    stats = vector_store.get_stats()
    
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("Documents", stats.get('n_documents', 0))
    with col2:
        st.metric("Categories", stats.get('n_categories', 0))
    with col3:
        st.metric("Status", stats.get('status', 'N/A'))
    
    st.markdown("---")
    
    # Tabs
    tab1, tab2 = st.tabs(["Query", "Pre-Compute"])
    
    with tab1:
        st.markdown("### Truy V·∫•n")
        
        if stats.get('n_documents', 0) == 0:
            st.warning("Ch∆∞a c√≥ d·ªØ li·ªáu. H√£y ch·∫°y Pre-Compute tr∆∞·ªõc.")
        else:
            query = st.text_input(
                "Nh·∫≠p c√¢u h·ªèi:", 
                placeholder="V√≠ d·ª•: Kh√°ch h√†ng nghƒ© g√¨ v·ªÅ ch·∫•t l∆∞·ª£ng √¢m thanh c·ªßa tai nghe?"
            )
            
            col1, col2 = st.columns(2)
            with col1:
                top_k = st.slider("S·ªë k·∫øt qu·∫£:", 1, 10, 5)
            with col2:
                use_llm = st.checkbox("S·ª≠ d·ª•ng LLM", value=True)
            
            if st.button("T√¨m Ki·∫øm", type="primary"):
                if query:
                    with st.spinner("ƒêang t√¨m ki·∫øm..."):
                        result = query_engine.query(query, top_k=top_k, use_llm=use_llm)
                        
                        if result.get('success'):
                            st.markdown("### Tr·∫£ L·ªùi")
                            st.markdown(result['response'])
                            
                            st.markdown("---")
                            st.markdown("### Aspects Li√™n Quan")
                            for asp in result.get('retrieved_aspects', []):
                                with st.expander(f"{asp['aspect_name']} ({asp['category']}) - Similarity: {asp['similarity']:.3f}"):
                                    st.markdown(f"**Summary:** {asp['summary']}")
                                    sentiment = asp.get('sentiment', {})
                                    st.markdown(f"**Sentiment:** {sentiment.get('interpretation', 'N/A')}")
                        else:
                            st.error(result.get('error', 'Query th·∫•t b·∫°i'))
                else:
                    st.warning("Vui l√≤ng nh·∫≠p c√¢u h·ªèi")
    
    with tab2:
        st.markdown("### Pre-Compute Aspects")
        st.markdown("Ph√¢n t√≠ch t·∫•t c·∫£ categories v√† l∆∞u v√†o vector store.")
        
        col1, col2 = st.columns(2)
        with col1:
            n_aspects = st.number_input("S·ªë aspects per category:", 3, 10, 5, key="rag_n_aspects")
        with col2:
            max_cats = st.number_input("Max categories (0=all):", 0, 100, 0, key="rag_max_cats")
        
        if st.button("Ch·∫°y Pre-Compute", type="primary", key="run_precompute"):
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            def update_progress(current, total, category):
                progress_bar.progress(current / total)
                status_text.text(f"[{current}/{total}] {category}")
            
            with st.spinner("ƒêang pre-compute..."):
                result = precompute_pipeline.run_full_pipeline(
                    n_aspects=int(n_aspects),
                    max_categories=int(max_cats) if max_cats > 0 else None,
                    progress_callback=update_progress
                )
                
                if result.get('success'):
                    st.success(f"Th√†nh c√¥ng! ƒê√£ x·ª≠ l√Ω {result['n_processed']} categories, {result['total_documents']} documents.")
                    
                    if result.get('failed_categories'):
                        st.warning(f"Th·∫•t b·∫°i: {len(result['failed_categories'])} categories")
                else:
                    st.error(result.get('error', 'Pre-compute th·∫•t b·∫°i'))


def render_full_evaluation(df):
    """Render full dataset evaluation section."""
    st.markdown("---")
    st.markdown("### ƒê√°nh Gi√° To√†n B·ªô Dataset")
    
    col1, col2 = st.columns(2)
    with col1:
        n_aspects = st.number_input("S·ªë aspects:", 3, 10, 5, key="full_eval_aspects")
    with col2:
        max_cats = st.number_input("Max categories (0=all):", 0, 50, 10, key="full_eval_cats")
    
    if st.button("Ch·∫°y Full Evaluation", type="secondary", key="run_full_eval"):
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        def update_progress(current, total, category):
            progress_bar.progress(current / total)
            status_text.text(f"[{current}/{total}] {category}")
        
        with st.spinner("ƒêang ƒë√°nh gi√° to√†n b·ªô dataset... (c√≥ th·ªÉ m·∫•t v√†i ph√∫t)"):
            try:
                from src.analysis.evaluator import full_dataset_evaluation
                
                result = full_dataset_evaluation(
                    df,
                    n_aspects=int(n_aspects),
                    max_categories=int(max_cats) if max_cats > 0 else None,
                    progress_callback=update_progress
                )
                
                if result.get('success'):
                    display_full_evaluation_results(result)
                else:
                    st.error(result.get('error', 'ƒê√°nh gi√° th·∫•t b·∫°i'))
                    
            except Exception as e:
                st.error(f"L·ªói: {e}")


def display_full_evaluation_results(result: dict):
    """Display full evaluation results."""
    st.markdown("---")
    st.markdown("## K·∫øt Qu·∫£ ƒê√°nh Gi√° To√†n Dataset")
    
    overall = result.get('overall', {})
    
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric("Overall Score", f"{overall.get('score', 0):.1%}")
    with col2:
        st.metric("Grade", overall.get('grade', 'N/A'))
    with col3:
        st.metric("Categories", result.get('categories_evaluated', 0))
    with col4:
        st.metric("Failed", result.get('categories_failed', 0))
    
    st.markdown("---")
    
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("Avg Silhouette", f"{overall.get('avg_silhouette', 0):.4f}")
    with col2:
        st.metric("Avg Coherence", f"{overall.get('avg_coherence', 0):.4f}")
    with col3:
        st.metric("Avg Coverage", f"{overall.get('avg_coverage', 0):.1f}%")
    
    # Best/Worst categories
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("### Top 5 Categories")
        for cat in result.get('best_categories', []):
            st.markdown(f"- **{cat['category']}**: {cat['overall_score']:.1%} ({cat['grade']})")
    
    with col2:
        st.markdown("### Bottom 5 Categories")
        for cat in result.get('worst_categories', []):
            st.markdown(f"- **{cat['category']}**: {cat['overall_score']:.1%} ({cat['grade']})")
    
    # Details table
    with st.expander("Chi ti·∫øt theo Category"):
        details_df = pd.DataFrame(result.get('category_details', []))
        if not details_df.empty:
            st.dataframe(details_df[['category', 'n_reviews', 'silhouette', 'coherence', 'coverage', 'overall_score', 'grade']], 
                        use_container_width=True, hide_index=True)


def main():
    """Main application."""
    df = load_data()
    
    if df is None:
        st.error("Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu. H√£y ch·∫°y pipeline tr∆∞·ªõc.")
        st.code("python main.py", language="bash")
        return
    
    summarizer = get_summarizer(df)
    page = render_sidebar()
    
    if page == "Bao Cao Du An":
        render_project_report(df)
    elif page == "Phan Tich Khia Canh":
        render_chatbot(df, summarizer)
    elif page == "RAG Query":
        render_rag_query(df)
    elif page == "Danh Gia Mo Hinh":
        render_evaluation(df, summarizer)
        render_full_evaluation(df)


if __name__ == "__main__":
    main()

